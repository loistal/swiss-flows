{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to design and play with the flow detection algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.append('../swiss_flows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from clean_tweets import clean_tweets\n",
    "\n",
    "# clean_tweets('../data/clean_tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the clean tweets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>userId</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>placeLongitude</th>\n",
       "      <th>placeLatitude</th>\n",
       "      <th>userLocation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>776522983837954049</td>\n",
       "      <td>735449229028675584</td>\n",
       "      <td>2016-09-15 20:48:01</td>\n",
       "      <td>8.96044</td>\n",
       "      <td>46.0027</td>\n",
       "      <td>Earleen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>776523000636203010</td>\n",
       "      <td>2741685639</td>\n",
       "      <td>2016-09-15 20:48:05</td>\n",
       "      <td>8.22414</td>\n",
       "      <td>46.8131</td>\n",
       "      <td>Suisse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>776523045200691200</td>\n",
       "      <td>435239151</td>\n",
       "      <td>2016-09-15 20:48:15</td>\n",
       "      <td>5.94082</td>\n",
       "      <td>47.2010</td>\n",
       "      <td>Fontain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>776523058404290560</td>\n",
       "      <td>503244217</td>\n",
       "      <td>2016-09-15 20:48:18</td>\n",
       "      <td>6.16552</td>\n",
       "      <td>45.8011</td>\n",
       "      <td>Shargeyah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>776523058504925185</td>\n",
       "      <td>452805259</td>\n",
       "      <td>2016-09-15 20:48:18</td>\n",
       "      <td>6.14319</td>\n",
       "      <td>46.2048</td>\n",
       "      <td>İstanbul/Burgazada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id              userId           createdAt  placeLongitude  \\\n",
       "0  776522983837954049  735449229028675584 2016-09-15 20:48:01         8.96044   \n",
       "1  776523000636203010          2741685639 2016-09-15 20:48:05         8.22414   \n",
       "2  776523045200691200           435239151 2016-09-15 20:48:15         5.94082   \n",
       "3  776523058404290560           503244217 2016-09-15 20:48:18         6.16552   \n",
       "4  776523058504925185           452805259 2016-09-15 20:48:18         6.14319   \n",
       "\n",
       "   placeLatitude        userLocation  \n",
       "0        46.0027           Earleen.   \n",
       "1        46.8131              Suisse  \n",
       "2        47.2010             Fontain  \n",
       "3        45.8011           Shargeyah  \n",
       "4        46.2048  İstanbul/Burgazada  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('../data/clean_tweets.csv', parse_dates=[2])\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping by user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to detect flows, we need to analyse the differents locations of people. This requires to analyse tweets by user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different users : 2763.\n"
     ]
    }
   ],
   "source": [
    "# Group by user\n",
    "grouped = tweets.groupby('userId')\n",
    "\n",
    "nb_user = len(tweets['userId'].value_counts())\n",
    "print('Number of different users : {}.'.format(nb_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many tweets by user do we get ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: The following cells have been commented because they cause the evaluation of the notebook (Run All) to be slow.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf = grouped.agg('count')['id'].reset_index().rename(columns={'id': 'tweets'}, index=str)\\ndf.head()\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df = grouped.agg('count')['id'].reset_index().rename(columns={'id': 'tweets'}, index=str)\n",
    "df.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf['tweets'].value_counts().plot(kind='bar')\\nplt.title('Distribution of tweets per user')\\nplt.xlabel('Number of tweets')\\nplt.ylabel('Number of user')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df['tweets'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of tweets per user')\n",
    "plt.xlabel('Number of tweets')\n",
    "plt.ylabel('Number of user')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it seems most of the users tweeted only once. This isn't very good for our ultimate goal. But we have to keep in mind that the actual dataset contains much more tweets, and so much more user that tweet more than once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users who only post a unique tweet provide no insight in the flows we wish to detect. Their corresponding tweets should be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many users do we initially have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2763"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data, grouped by user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for name, group in grouped:\\n    print(name)\\n    print(group)\\n    print('--------------------------------------------------------------------')\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grouped is a DataFrameGroupBy object, it cannot be displayed like a dataframe\n",
    "\"\"\"for name, group in grouped:\n",
    "    print(name)\n",
    "    print(group)\n",
    "    print('--------------------------------------------------------------------')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We filter out the users who only have 1 one tweet. \n",
    "- The object we currently have is a DataFrameGroupBy object, which is unusual and hard to manipulate. Hence, we convert it to a dictionary.\n",
    "- Since the keys of this dictionary correspond to the userId, we no longer need the ```userId``` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform the DataFrameGroupBy object into a dictionary\n",
    "user_tweets = {}\n",
    "for name, group in grouped:\n",
    "    \n",
    "    # Filter out users with less than 1 tweet\n",
    "    if(group.shape[0] > 1):\n",
    "        \n",
    "        # Remove the userId column since we use it as key\n",
    "        user_tweets[name] = group.drop('userId', axis=1)\n",
    "        \n",
    "#user_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time interval condition\n",
    "\n",
    "Tweets which are excessively spaced out in time are not viable. In other words, tweet emitted at time $t$ by a given user should be removed if this user hasn't emitted another tweet within the interval $[t - l/2, t + l/2]$, where $l$ is a set duration.\n",
    "\n",
    "Therefore, for each user, we create all pairs of tweets which respect the above criterion, since they may represent a potential flow. We remove tweets which cannot pair with another tweet from the same user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node condition\n",
    "\n",
    "The pairs generated represent potential flows only if the tweets were emitted from different Nodes. Consequently, we remove pairs where:\n",
    "- one or both tweet(s) do(es) not reside in a Node\n",
    "- both tweets were emitted from the same Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetrical flows\n",
    "If flows are not directed : A <--> B is equivalent to B <--> A. We normalize undirected flows by lexicographical order of their nodes **in the constructor**.\n",
    "\n",
    "Directed flows can't be normalized as they define a precise direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlapping flows\n",
    "\n",
    "Note that there is a problem here. Some users have an incredible number of pairs which potentially represent flows. However, a lot of those pairs represent the exact same flow. Indeed, certain users post several tweets at the same location. Therefore, a user may have several pairs representing the same flow, during the same period. We should only keep one of those pairs.\n",
    "\n",
    "#### Example \n",
    "\n",
    "Consider this sequence of tweet locations : A -> A -> B with the 2 first tweets being posted in the same hour. If we naively consider all tweet pairs we will generate 2 undirected flows A -> B and A -> B, which actually represent the same logical tweet.\n",
    "\n",
    "A solution could be the following : each time we generate a flow, we associate to it the time of the 2 tweets we used. If an identical flow (or symmetrical for undirected flows) overlaps, we should just keep one of those flows.\n",
    "\n",
    "#### Problem\n",
    "\n",
    "There is something we need to notice we trying to detect overlapping flows, consider the following time sequence: `A1 - B1 - A2 - B2`, with `Ax` and `Bx` representing the locations `A` and `B`.\n",
    "\n",
    "Depending on the order we treat the tweet pairs, we will get different results: \n",
    "* `A1 - B1` then `B1 - A2`... finally `A1 - B2`: the 3 small intervals will be counted as **3 different moves** as they don't overlap, however the last one `A1-B2` won't be count since it overlaps with the others.\n",
    "* `A1 - B2` first, then the others: the first big one will be counted as **1 move**, and the others will be ignored as they overlap with the first interval.\n",
    "* We could probably find other results with other orders...\n",
    "\n",
    "So we have an ordering problem here.\n",
    "\n",
    "**Proposed solution**:\n",
    "\n",
    "By looking at our example, it is clear that in our context, the true version we want to get is the first one, as moves with smaller intervals logically represent a flow and moves with bigger intervals represent a flow only if they don't overlap with smaller ones.\n",
    "\n",
    "The idea is to simply treat small intervals first. Then when it comes to bigger ones, we will count them only if they don't overlap with small ones and get a deterministic result.\n",
    "\n",
    "This is done by sorting the list of pairs for each user by interval length, and going through this list in order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting flows\n",
    "Of course if two identical flows don't overlap, they shouldn't be considered are the exact same flow in time, so we should attribute a greater weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directed flows\n",
    "\n",
    "The problem here is to try to identify if flows have directions, i.e. instead of saying that people are moving _between_ A and B, being able to say that people are moving _from_ A _to_ B.\n",
    "\n",
    "#### Strategy\n",
    "\n",
    "Consider the following sequence `A -> B -> A -> B`.\n",
    "\n",
    "What should we conclude about the flows ? We identify two strategies: \n",
    "\n",
    "1. Someone is moving from A to B, and we should only count the flow `A -> B`.\n",
    "2. Someone is moving from A to B and from B to A, and we should count both `A -> B` and `B -> A`.\n",
    "\n",
    "**Strategy 1**:\n",
    "\n",
    "This strategy seems to be the most logical one, since in this example we intuitively see that A is the starting point. But this is actually wrong, it turns out that even if we have a tweet from A first, it doesn't mean that the person is starting from A. Imagine we have one tweet from B right before : `B -> A -> B -> A -> B`, it completely changes the meaning we consider this strategy...\n",
    "\n",
    "**Strategy 2**:\n",
    "\n",
    "This is the naive strategy, i.e., basically counting the pairs of tweets and associating the earliest one to the starting point.\n",
    "\n",
    "One can ask itself how does this fundamentally change from undirected flows, as we just decompose flows naively. Well, there are mutliple things to consider:\n",
    "\n",
    "* If we count 2 flows `A -> B` and only 1 `B -> A`, it might be that some other flows, let's say `C -> D` comes with an weight of 2, and makes it more important than `B -> A`, which couldn't happen with undirected flows.\n",
    "* We can also bring rafinements to the detection process, for example with the pattern above, if we have `B (11pm) -> A (8am)`, we can maybe deduce that B is actually not the starting point.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "The strategy which seems the most reasonnable is the second one, with the following choices: \n",
    "\n",
    "* The starting point is infered from the tweet that preceeds the other.\n",
    "* A tweet preceeds another if it was posted logically before in time **and** it was posted earlier considering only the time of the day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../swiss_flows/node.py:262: UserWarning: File already exists, importing ...\n",
      "  warnings.warn('File already exists, importing ...', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from node import Node\n",
    "\n",
    "# Generate the nodes\n",
    "nodes = Node.generate_nodes(n_swiss_nodes=10, n_foreign_nodes=1, pop_threshold=15000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def by_interval_len(tweet_tuple):\n",
    "    tmp1 = tweet_tuple[0]\n",
    "    tmp2 = tweet_tuple[1]\n",
    "    \n",
    "    # Order the tweet by timestamp\n",
    "    t1 = tmp1 if tmp1[1].to_pydatetime() < tmp2[1].to_pydatetime() else tmp2\n",
    "    t2 = tmp2 if tmp1[1].to_pydatetime() < tmp2[1].to_pydatetime() else tmp1\n",
    "    \n",
    "    # Return the length of the interval\n",
    "    return t2[1].to_pydatetime() - t1[1].to_pydatetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from flow import Flow\n",
    "import itertools\n",
    "\n",
    "# Duration in days\n",
    "l = 2\n",
    "\n",
    "# Detect directed or undirect flows\n",
    "directed = True\n",
    "\n",
    "user_flows = {}\n",
    "for user_id, tweet_info in user_tweets.items():\n",
    "    \n",
    "    # Generate all possible pairs of tweet sorted by interval length\n",
    "    pairs = sorted(list(itertools.combinations(tweet_info.values.tolist(), 2)), key=by_interval_len)\n",
    "\n",
    "    # {f1 : {weight:1, intervals:[interval1, interval2...]}}\n",
    "    flows = {}\n",
    "    for id_pair in pairs:\n",
    "        \n",
    "        # [id, Timestamp, lon, lat, location]\n",
    "        t1 = id_pair[0]\n",
    "        t2 = id_pair[1]\n",
    "                \n",
    "        # Nodes corresponding to the tweets\n",
    "        n1 = Node.locate_point((t1[3], t1[2]), nodes)\n",
    "        n2 = Node.locate_point((t2[3], t2[2]), nodes)\n",
    "                \n",
    "        # Time interval condition\n",
    "        time1 = t1[1].to_pydatetime()\n",
    "        time2 = t2[1].to_pydatetime()\n",
    "        ts1 = time1 if time1 < time2 else time2\n",
    "        ts2 = time2 if time1 < time2 else time1\n",
    "        tweet_interval = (ts1, ts2)\n",
    "        time_cond = (ts2 - ts1).days <= l\n",
    "        \n",
    "        # Node conditions\n",
    "        geo_cond = n1 and n2 and (n1 != n2)\n",
    "                \n",
    "        if time_cond and geo_cond:\n",
    "            # Build the flow\n",
    "            src = n1\n",
    "            dst = n2\n",
    "            \n",
    "            if directed: \n",
    "                if time1 < time2 and time1.time() < time2.time():\n",
    "                    src = n1\n",
    "                    dst = n2\n",
    "                elif time2 < time1 and time2.time() < time1.time():\n",
    "                    src = n2\n",
    "                    dst = n1\n",
    "                else:\n",
    "                    # Cannot conclude\n",
    "                    continue\n",
    "                    \n",
    "            flow = Flow(src=src, dst=dst, directed=directed)\n",
    "                        \n",
    "            overlap = False\n",
    "            if flow in flows:\n",
    "                # Look for overlapping flows\n",
    "                for interval in flows[flow]['intervals']:\n",
    "                    if Flow.is_overlapping(tweet_interval, interval):\n",
    "                        overlap = True\n",
    "                        break\n",
    "            \n",
    "            else:\n",
    "                # Add the initial values if it's a new flow\n",
    "                flows[flow] = {'weight': 1, 'intervals': [], 'start':ts1, 'end':ts2}\n",
    "            \n",
    "            # If no overlap, then it's not the exact same flow\n",
    "            if not overlap:\n",
    "                # Update start date\n",
    "                flows[flow]['start'] = min(ts1, flows[flow]['start'])\n",
    "                \n",
    "                # Update end date\n",
    "                flows[flow]['end'] = max(ts2, flows[flow]['end'])\n",
    "                \n",
    "                # Update weight\n",
    "                flows[flow]['weight'] += 1\n",
    "                \n",
    "            # In any case, add the interval we just found for later use\n",
    "            flows[flow]['intervals'].append(tweet_interval)\n",
    "    \n",
    "    # Save those flows\n",
    "    if len(flows) > 0:\n",
    "        user_flows[user_id] = flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get rid of the interval lists and associate each flow with its weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.339124140427072% of the users define a flow.\n"
     ]
    }
   ],
   "source": [
    "print('{}% of the users define a flow.'.format(len(user_flows)*100/len(grouped)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty low... but we can expect to get something more reliable with the whole set of tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating flows\n",
    "We need to aggregate flows infered from different users in order to define the importance of each flow, and update the dates.\n",
    "\n",
    "**NOTE**: we can integrate this step in the detection phase.\n",
    "\n",
    "We tried to aggregate flows using `pandas` as it's probably more efficient. It turns out we need ordering amont flows which doesn't really make sense here. Given the amount of flows is limited, it's still reasonnable to aggregate greedily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aggregate the flows\n",
    "agg_flows = {}\n",
    "\n",
    "for user, flows in user_flows.items():\n",
    "    for flow, attr in flows.items():\n",
    "        if flow not in agg_flows:\n",
    "            agg_flows[flow] = {'weight': attr['weight'], 'start': attr['start'], 'end': attr['end']}\n",
    "        else:\n",
    "            agg_flows[flow]['weight'] += attr['weight']\n",
    "            agg_flows[flow]['start'] = min(agg_flows[flow]['start'], attr['start'])\n",
    "            agg_flows[flow]['end'] = min(agg_flows[flow]['end'], attr['end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Update the weight attribute of each flow\n",
    "final_flows = []\n",
    "for flow, attr in agg_flows.items():\n",
    "    flow.weight = attr['weight']\n",
    "    flow.start_date = attr['start']\n",
    "    flow.end_date = attr['end']\n",
    "    final_flows.append(flow)\n",
    "    \n",
    "# Sort it by weight\n",
    "final_flows.sort(key=lambda x: x.weight, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 flows.\n",
      "[Flow] Bern --> Winterthur (weight: 36, start: 2016-09-16 02:05:04, end: 2016-09-16 03:45:05).\n",
      "[Flow] Bern --> Geneve (weight: 11, start: 2016-09-16 02:05:04, end: 2016-09-16 11:07:16).\n",
      "[Flow] Lausanne --> Geneve (weight: 10, start: 2016-09-16 00:25:46, end: 2016-09-16 08:52:09).\n",
      "[Flow] Geneve --> Zurich (weight: 9, start: 2016-09-16 06:08:16, end: 2016-09-16 09:30:25).\n",
      "[Flow] Bern --> Basel (weight: 8, start: 2016-09-16 02:05:04, end: 2016-09-16 04:45:14).\n",
      "[Flow] Luzern --> Zurich (weight: 7, start: 2016-09-16 05:21:47, end: 2016-09-16 10:31:17).\n",
      "[Flow] Zurich --> Luzern (weight: 7, start: 2016-09-16 05:50:59, end: 2016-09-16 07:55:03).\n",
      "[Flow] Zurich --> Winterthur (weight: 7, start: 2016-09-16 08:13:25, end: 2016-09-16 13:39:51).\n",
      "[Flow] Basel --> Zurich (weight: 6, start: 2016-09-16 04:45:14, end: 2016-09-16 12:35:53).\n",
      "[Flow] Bern --> Zurich (weight: 6, start: 2016-09-16 02:05:04, end: 2016-09-16 13:36:30).\n",
      "[Flow] Lausanne --> Bern (weight: 6, start: 2016-09-16 09:33:00, end: 2016-09-16 12:51:53).\n",
      "[Flow] Saint-Louis --> Basel (weight: 6, start: 2016-09-16 07:00:01, end: 2016-09-16 16:12:24).\n",
      "[Flow] Basel --> Saint-Louis (weight: 6, start: 2016-09-16 04:49:36, end: 2016-09-16 11:37:13).\n",
      "[Flow] Zurich --> Bern (weight: 6, start: 2016-09-16 11:08:02, end: 2016-09-16 13:18:00).\n",
      "[Flow] Winterthur --> Zurich (weight: 6, start: 2016-09-16 03:45:05, end: 2016-09-16 15:25:24).\n",
      "[Flow] Biel/Bienne --> Bern (weight: 4, start: 2016-09-16 04:17:30, end: 2016-09-16 05:33:58).\n",
      "[Flow] Sankt Gallen --> Zurich (weight: 4, start: 2016-09-16 06:30:19, end: 2016-09-16 13:07:28).\n",
      "[Flow] Geneve --> Bern (weight: 4, start: 2016-09-16 08:22:13, end: 2016-09-16 10:14:20).\n",
      "[Flow] Bern --> Biel/Bienne (weight: 4, start: 2016-09-16 02:05:04, end: 2016-09-16 12:40:04).\n",
      "[Flow] Basel --> Luzern (weight: 4, start: 2016-09-15 23:36:25, end: 2016-09-15 23:38:01).\n",
      "[Flow] Basel --> Sankt Gallen (weight: 4, start: 2016-09-16 04:45:14, end: 2016-09-16 06:30:19).\n",
      "[Flow] Geneve --> Lausanne (weight: 4, start: 2016-09-16 07:56:49, end: 2016-09-16 14:55:12).\n",
      "[Flow] Winterthur --> Luzern (weight: 4, start: 2016-09-16 03:45:05, end: 2016-09-16 12:02:54).\n",
      "[Flow] Basel --> Geneve (weight: 3, start: 2016-09-16 04:45:14, end: 2016-09-16 16:05:02).\n",
      "[Flow] Luzern --> Basel (weight: 3, start: 2016-09-15 23:32:09, end: 2016-09-15 23:36:25).\n",
      "[Flow] Winterthur --> Geneve (weight: 3, start: 2016-09-16 03:45:05, end: 2016-09-16 16:05:02).\n",
      "[Flow] Biel/Bienne --> Geneve (weight: 3, start: 2016-09-16 12:40:04, end: 2016-09-16 16:05:02).\n",
      "[Flow] Winterthur --> Basel (weight: 3, start: 2016-09-16 03:45:05, end: 2016-09-16 04:45:14).\n",
      "[Flow] Domodossola --> Geneve (weight: 3, start: 2016-09-16 06:05:07, end: 2016-09-16 16:05:02).\n",
      "[Flow] Sankt Gallen --> Geneve (weight: 3, start: 2016-09-16 06:30:19, end: 2016-09-16 16:05:02).\n",
      "[Flow] Lausanne --> Basel (weight: 3, start: 2016-09-16 12:11:18, end: 2016-09-16 14:30:19).\n",
      "[Flow] Zurich --> Basel (weight: 2, start: 2016-09-16 10:37:03, end: 2016-09-16 11:38:00).\n",
      "[Flow] Zurich --> Lausanne (weight: 2, start: 2016-09-16 06:26:11, end: 2016-09-16 09:00:40).\n",
      "[Flow] Domodossola --> Biel/Bienne (weight: 2, start: 2016-09-16 06:05:07, end: 2016-09-16 12:40:04).\n",
      "[Flow] Sankt Gallen --> Luzern (weight: 2, start: 2016-09-16 06:30:19, end: 2016-09-16 16:20:02).\n",
      "[Flow] Bern --> Sankt Gallen (weight: 2, start: 2016-09-16 02:05:04, end: 2016-09-16 06:30:19).\n",
      "[Flow] Domodossola --> Bern (weight: 2, start: 2016-09-16 06:05:07, end: 2016-09-16 16:20:04).\n",
      "[Flow] Basel --> Biel/Bienne (weight: 2, start: 2016-09-16 04:45:14, end: 2016-09-16 12:40:04).\n",
      "[Flow] Winterthur --> Domodossola (weight: 2, start: 2016-09-16 03:45:05, end: 2016-09-16 06:05:07).\n",
      "[Flow] Zurich --> Sankt Gallen (weight: 2, start: 2016-09-16 10:37:03, end: 2016-09-16 13:52:02).\n",
      "[Flow] Basel --> Domodossola (weight: 2, start: 2016-09-16 04:45:14, end: 2016-09-16 06:05:07).\n",
      "[Flow] Winterthur --> Sankt Gallen (weight: 2, start: 2016-09-16 03:45:05, end: 2016-09-16 06:30:19).\n",
      "[Flow] Domodossola --> Luzern (weight: 2, start: 2016-09-16 06:05:07, end: 2016-09-16 16:20:02).\n",
      "[Flow] Domodossola --> Zurich (weight: 2, start: 2016-09-16 06:05:07, end: 2016-09-16 16:20:04).\n",
      "[Flow] Sankt Gallen --> Bern (weight: 2, start: 2016-09-16 06:30:19, end: 2016-09-16 16:20:04).\n",
      "[Flow] Bern --> Domodossola (weight: 2, start: 2016-09-16 02:05:04, end: 2016-09-16 06:05:07).\n",
      "[Flow] Basel --> Bern (weight: 2, start: 2016-09-16 04:45:14, end: 2016-09-16 16:20:04).\n",
      "[Flow] Sankt Gallen --> Biel/Bienne (weight: 2, start: 2016-09-16 06:30:19, end: 2016-09-16 12:40:04).\n",
      "[Flow] Geneve --> Luzern (weight: 2, start: 2016-09-16 16:05:02, end: 2016-09-16 16:20:02).\n",
      "[Flow] Biel/Bienne --> Zurich (weight: 2, start: 2016-09-16 12:40:04, end: 2016-09-16 16:20:04).\n",
      "[Flow] Domodossola --> Sankt Gallen (weight: 2, start: 2016-09-16 06:05:07, end: 2016-09-16 06:30:19).\n",
      "[Flow] Luzern --> Bern (weight: 2, start: 2016-09-16 16:20:02, end: 2016-09-16 16:20:04).\n",
      "[Flow] Lausanne --> Zurich (weight: 2, start: 2016-09-16 12:48:52, end: 2016-09-16 12:54:50).\n",
      "[Flow] Winterthur --> Biel/Bienne (weight: 2, start: 2016-09-16 03:45:05, end: 2016-09-16 12:40:04).\n",
      "[Flow] Bern --> Luzern (weight: 2, start: 2016-09-16 02:05:04, end: 2016-09-16 16:20:02).\n",
      "[Flow] Biel/Bienne --> Luzern (weight: 2, start: 2016-09-16 12:40:04, end: 2016-09-16 16:20:02).\n",
      "[Flow] Winterthur --> Bern (weight: 2, start: 2016-09-16 03:45:05, end: 2016-09-16 16:20:04).\n"
     ]
    }
   ],
   "source": [
    "print('{} flows.'.format(len(final_flows)))\n",
    "\n",
    "for flow in final_flows:\n",
    "    print(flow)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
